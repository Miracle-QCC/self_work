{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QBFvvcU05V-X",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/HumanCompatibleAI/overcooked_ai.git\n",
      "  Cloning https://github.com/HumanCompatibleAI/overcooked_ai.git to c:\\users\\qin\\appdata\\local\\temp\\pip-req-build-jrie3g3v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/HumanCompatibleAI/overcooked_ai.git 'C:\\Users\\qin\\AppData\\Local\\Temp\\pip-req-build-jrie3g3v'\n",
      "  fatal: unable to access 'https://github.com/HumanCompatibleAI/overcooked_ai.git/': Failed to connect to github.com port 443 after 21081 ms: Timed out\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  git clone --filter=blob:none --quiet https://github.com/HumanCompatibleAI/overcooked_ai.git 'C:\\Users\\qin\\AppData\\Local\\Temp\\pip-req-build-jrie3g3v' did not run successfully.\n",
      "  exit code: 128\n",
      "  \n",
      "  See above for output.\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "git clone --filter=blob:none --quiet https://github.com/HumanCompatibleAI/overcooked_ai.git 'C:\\Users\\qin\\AppData\\Local\\Temp\\pip-req-build-jrie3g3v' did not run successfully.\n",
      "exit code: 128\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "### Install libraries ###\n",
    "\n",
    "!pip install git+https://github.com/HumanCompatibleAI/overcooked_ai.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScQhFpxT5ZkL"
   },
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "\n",
    "from overcooked_ai_py.mdp.overcooked_mdp import OvercookedGridworld\n",
    "from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv\n",
    "from overcooked_ai_py.agents.benchmarking import AgentEvaluator\n",
    "from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\n",
    "from overcooked_ai_py.agents.agent import NNPolicy, AgentFromPolicy, AgentPair\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from IPython.display import display, Image as IPImage\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aAr6oW-Y5b3C",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OvercookedGridworld' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13072\\688489637.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Build the environment.  Do not modify!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mmdp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOvercookedGridworld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_layout_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrew_shaping_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreward_shaping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mbase_env\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOvercookedEnv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_mdp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmdp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhorizon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo_level\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m env = gym.make(\"Overcooked-v0\", base_env=base_env,\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OvercookedGridworld' is not defined"
     ]
    }
   ],
   "source": [
    "### Environment setup ###\n",
    "\n",
    "# Swap between the 5 layouts here:\n",
    "layout = \"cramped_room\"\n",
    "# layout = \"asymmetric_advantages\"\n",
    "# layout = \"coordination_ring\"\n",
    "# layout = \"forced_coordination\"\n",
    "# layout = \"counter_circuit_o_1order\"\n",
    "\n",
    "# Reward shaping is disabled by default.  This data structure may be used for\n",
    "# reward shaping.  You can, of course, do your own reward shaping in lieu of, or\n",
    "# in addition to, using this structure.\n",
    "reward_shaping = {\n",
    "    \"PLACEMENT_IN_POT_REW\": 3,\n",
    "    \"DISH_PICKUP_REWARD\": 3,\n",
    "    \"SOUP_PICKUP_REWARD\": 5\n",
    "}\n",
    "\n",
    "# Length of Episodes.  Do not modify for your submission!\n",
    "# Modification will result in a grading penalty!\n",
    "horizon = 400\n",
    "\n",
    "# Build the environment.  Do not modify!\n",
    "mdp = OvercookedGridworld.from_layout_name(layout, rew_shaping_params=reward_shaping)\n",
    "base_env = OvercookedEnv.from_mdp(mdp, horizon=horizon, info_level=0)\n",
    "env = gym.make(\"Overcooked-v0\", base_env=base_env,\n",
    "               featurize_fn=base_env.featurize_state_mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8EsQPGw35fKS"
   },
   "outputs": [],
   "source": [
    "### Train your agent ###\n",
    "\n",
    "# The code below runs a few episodes with a random agent.  Your learning algorithm\n",
    "# would go here.\n",
    "\n",
    "num_episodes = 5\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    # Episode termination flag\n",
    "    done = False\n",
    "\n",
    "    # The number of soups the agent pair made during the episode\n",
    "    num_soups_made = 0\n",
    "\n",
    "    # Reset the environment at the start of each episode\n",
    "    obs = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        # Obtain observations for each agent\n",
    "        obs0 = obs[\"both_agent_obs\"][0]\n",
    "        obs1 = obs[\"both_agent_obs\"][1]\n",
    "\n",
    "        # Select random actions from the set {North, South, East, West, Stay, Interact}\n",
    "        # for each agent.\n",
    "        a0 = env.action_space.sample()\n",
    "        a1 = env.action_space.sample()\n",
    "\n",
    "        # Take the selected actions and receive feedback from the environment\n",
    "        # The returned reward \"R\" only reflects completed soups.  You can find\n",
    "        # the separate shaping rewards in the \"info\" variables\n",
    "        # info[\"shaped_r_by_agent\"][0] and info[\"shaped_r_by_agent\"][1].  Note that\n",
    "        # this shaping reward does *not* include the +20 reward for completed\n",
    "        # soups returned in \"R\".\n",
    "        obs, R, done, info = env.step([a0, a1])\n",
    "\n",
    "        # Accumulate the number of soups made\n",
    "        num_soups_made += int(R / 20) # Each served soup generates 20 reward\n",
    "\n",
    "    # Display status\n",
    "    print(\"Ep {0}\".format(e + 1), end=\" \")\n",
    "    print(\"number of soups made: {0}\".format(num_soups_made))\n",
    "\n",
    "# The info flag returned by the environemnt contains special status info\n",
    "# specifically when done == True.  This information may be useful in\n",
    "# developing, debugging, and analyzing your results.  It may also be a good\n",
    "# way for you to find a metric that you can use in evaluating collaboration\n",
    "# between your agents.\n",
    "print(\"\\nExample info dump:\\n\\n\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GpPoWGp5hzZ"
   },
   "outputs": [],
   "source": [
    "### Evaluate your agent ###\n",
    "\n",
    "# This is where you would rollout episodes with your trained agent.\n",
    "# The below code is a partcular way to rollout episodes in a format\n",
    "# compatible with a state visualizer, if you'd like to visualize what your\n",
    "# agents are doing during episodes.  Visualization is in the next cell.\n",
    "\n",
    "class StudentPolicy(NNPolicy):\n",
    "    \"\"\" Generate policy \"\"\"\n",
    "    def __init__(self):\n",
    "        super(StudentPolicy, self).__init__()\n",
    "\n",
    "    def state_policy(self, state, agent_index):\n",
    "        \"\"\"\n",
    "        This method should be used to generate the poiicy vector corresponding to\n",
    "        the state and agent_index provided as input.  If you're using a neural\n",
    "        network-based solution, the specifics depend on the algorithm you are using.\n",
    "        Below are two commented examples, the first for a policy gradient algorithm\n",
    "        and the second for a value-based algorithm.  In policy gradient algorithms,\n",
    "        the neural networks output a policy directly.  In value-based algorithms,\n",
    "        the policy must be derived from the Q value outputs of the networks.  The\n",
    "        uncommented code below is a placeholder that generates a random policy.\n",
    "        \"\"\"\n",
    "        featurized_state = base_env.featurize_state_mdp(state)\n",
    "        input_state = torch.FloatTensor(featurized_state[agent_index]).unsqueeze(0)\n",
    "\n",
    "        # Example for policy NNs named \"PNN0\" and \"PNN1\"\n",
    "        # with torch.no_grad():\n",
    "        #   if agent_index == 0:\n",
    "        #       action_probs = PNN0(input_state)[0].numpy()\n",
    "        #   else:\n",
    "        #       action_probs = PNN1(input_state)[0].numpy()\n",
    "\n",
    "        # Example for Q value NNs named \"QNN0\" and \"QNN1\"\n",
    "        # action_probs = np.zeros(env.action_space.n)\n",
    "        # with torch.no_grad():\n",
    "        #   if agent_index == 0:\n",
    "        #       action_probs[np.argmax(QNN0(input_state)[0].numpy())] = 1\n",
    "        #   else:\n",
    "        #       action_probs[np.argmax(QNN1(input_state)[0].numpy())] = 1\n",
    "\n",
    "        # Random deterministic policy\n",
    "        action_probs = np.zeros(env.action_space.n)\n",
    "        action_probs[env.action_space.sample()] = 1\n",
    "\n",
    "        return action_probs\n",
    "\n",
    "    def multi_state_policy(self, states, agent_indices):\n",
    "        \"\"\" Generate a policy for a list of states and agent indices \"\"\"\n",
    "        return [self.state_policy(state, agent_index) for state, agent_index in zip(states, agent_indices)]\n",
    "\n",
    "\n",
    "class StudentAgent(AgentFromPolicy):\n",
    "    \"\"\"Create an agent using the policy created by the class above\"\"\"\n",
    "    def __init__(self, policy):\n",
    "        super(StudentAgent, self).__init__(policy)\n",
    "\n",
    "\n",
    "# Instantiate the policies for both agents\n",
    "policy0 = StudentPolicy()\n",
    "policy1 = StudentPolicy()\n",
    "\n",
    "# Instantiate both agents\n",
    "agent0 = StudentAgent(policy0)\n",
    "agent1 = StudentAgent(policy1)\n",
    "agent_pair = AgentPair(agent0, agent1)\n",
    "\n",
    "# Generate an episode\n",
    "ae = AgentEvaluator.from_layout_name({\"layout_name\": layout}, {\"horizon\": horizon})\n",
    "trajs = ae.evaluate_agent_pair(agent_pair, num_games=1)\n",
    "print(\"\\nlen(trajs):\", len(trajs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1ZvcY9d5lhy"
   },
   "outputs": [],
   "source": [
    "### Agent Visualization ###\n",
    "\n",
    "##############################################################################\n",
    "# The function StateVisualizer() below generates images for the state of the\n",
    "# environment at each time step of the episode.\n",
    "#\n",
    "# You have several options for how to use these images:\n",
    "#\n",
    "# 1) You can set img_dir to a local directory (or a directory within Google Drive\n",
    "# if using Colab), and all the images will be saved to that directory for you to browse.\n",
    "#\n",
    "# 2) If using a notebook, you can set the argument ipthon_display=True to get a\n",
    "# tool with a slider that lets you scan through all the images directly in the\n",
    "# notebook.  This option does not require you to store your images.\n",
    "#\n",
    "# 3) You can generate a GIF of the episode. This requires you to set\n",
    "# img_dir.  The code to generate the GIF is commented out below\n",
    "\n",
    "# Modify as appropriate\n",
    "img_dir = None\n",
    "ipython_display = True\n",
    "gif_path = None\n",
    "\n",
    "# Do not modify -- uncomment for GIF generation\n",
    "StateVisualizer().display_rendered_trajectory(trajs, img_directory_path=img_dir, ipython_display=ipython_display)\n",
    "# img_list = [f for f in os.listdir(img_dir) if f.endswith('.png')]\n",
    "# img_list.sort(key=lambda x: os.path.getmtime(os.path.join(img_dir, x)))\n",
    "# images = [Image.open(img_dir + img).convert('RGBA') for img in img_list]\n",
    "# images[0].save(gif_path, save_all=True, append_images=images[1:], optimize=False, duration=250, loop=0)\n",
    "# with open(gif_path,'rb') as f: display(IPImage(data=f.read(), format='png'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPvD5jSPp+kwOFfA/X3s/LE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
